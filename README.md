# Individual project3 from IDS721
This is a course based project from IDS721, and the main task of this project is to use a major Big Data system to perform a Data Engineering related task.

## Description
This project is to build a data pipeline that extracts data from various sources, transforms it into a standardized format, and loads it into a data warehouse. Specifically, we will be working with data from a fictitious e-commerce website that sells products across multiple regions. The data includes sales records, customer data, and product data.

I will be using AWS Spark to perform the ETL (extract, transform, load) process. We will also be using AWS services such as S3 for storage and Glue for data catalog and ETL job orchestration.

## Get started
To get started with this project, you will need to have access to an AWS account. You will also need to have the necessary permissions to create S3 buckets, Glue jobs, and other AWS services.

1. Clone the repository to your local machine
2. Create an S3 bucket to store your data
3. Set up an AWS Glue data catalog
4. Create AWS Glue jobs to perform the ETL process
5. Use AWS Spark to transform the data and load it into a data warehouse
6. Automate the ETL process using AWS Glue scheduling

## Weekly plan
1. Week 1: Data Collection and Preparation
Identify and collect data from various sources. Create a data schema for the data. Clean and prepare the data for the ETL process

2. Week 2: ETL Process and Data Warehouse
Create AWS Glue jobs to perform the ETL process. Use AWS Spark to transform the data into a standardized format. Load the transformed data into a data warehouse. Perform data quality checks to ensure accuracy
3. Week 3: Automation and Final Touches
Automate the ETL process using AWS Glue scheduling. Create visualizations and reports for the data. Add any final touches and perform code cleanup
## References

* [rust-cli-template](https://github.com/kbknapp/rust-cli-template)
